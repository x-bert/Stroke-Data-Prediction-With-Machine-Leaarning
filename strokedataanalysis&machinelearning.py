# -*- coding: utf-8 -*-
"""StrokeDataAnalysis&MachineLearning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ExH8W1zS4nAsijgvfgSq1z0mEtg28TzJ

# Stroke Data Analysis

Data berasal dari kaggle.

Link data: https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset
"""

import numpy as np
import pandas as pd
from scipy.stats import chi2_contingency
import matplotlib.pyplot as plt
from scipy.stats import pearsonr
import seaborn as sns

stroke_data = pd.read_csv('stroke-data.csv',sep=";")
stroke_data.head()

"""variabel kategorik: gender, ever_married, work_type, Residence_type, smoking_status, hypertension, heart_disease, stroke
variabel numerik: id, age, avg_glucose_level, bmi

Pada Hypertension, Heart_disease, Stroke data 0, 1 diartikan sebagai:
- 0 : Tidak (No)
- 1 : Iya (Yes)

## Data Checking and Cleaning

Pada bagian ini kita akan mengecek data dan membersihkannya apabila perlu.

### 1.Univariant: Variabel Kategorik (Pie Chart)

Pie Chart Gender
"""

t1 = stroke_data['gender'].value_counts()

print(t1)

labels = t1.index
sizes = t1.values

colors = ['#f756a3','#165baa']
plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors)
plt.title('Gender')
plt.show()

"""Terdapat varibel error, Other, maka akan dihapuskan."""

cleaned_data = stroke_data[stroke_data['gender'] != 'Other']

t2 = cleaned_data['gender'].value_counts()
print(t2)

labels = t2.index
sizes = t2.values

colors = ['#f756a3','#165baa']
plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors)
plt.title('Gender')
plt.show()

"""Pie Chart Martial Status (ever_married)

"""

t3 = cleaned_data['ever_married'].value_counts()

print(t3)

labels = t3.index
sizes = t3.values

colors = ['#00a54c','#d40028']
plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors)
plt.title('Martial Status')
plt.show()

"""Pie Chart Work Type"""

t4 = cleaned_data['work_type'].value_counts()

print(t4)

labels = t4.index
sizes = t4.values

plt.pie(sizes, labels=labels, autopct='%1.1f%%')
plt.title('Work Type')
plt.show()

"""Pie Chart Residence Type"""

t5 = cleaned_data['Residence_type'].value_counts()

print(t5)

labels = t5.index
sizes = t5.values

plt.pie(sizes, labels=labels, autopct='%1.1f%%')
plt.title('Residence Type')
plt.show()

"""Pie Chart Smoking Status"""

t6 = cleaned_data['smoking_status'].value_counts()

print(t6)

labels = t6.index
sizes = t6.values

plt.pie(sizes, labels=labels, autopct='%1.1f%%')
plt.title('Smoking Status')
plt.show()

"""Pie Chart Hypertension"""

t7 = cleaned_data['hypertension'].value_counts()

print(t7)

label_map = {0: 'No', 1: 'Yes'}

labels = [label_map.get(i, i) for i in t7.index]
sizes = t7.values

colors = ['#d40028', '#00a54c']

plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors)
plt.title('Hypertension')
plt.show()

"""Pie Chart Heart Disease"""

t8 = cleaned_data['heart_disease'].value_counts()

print(t8)

label_map = {0: 'No', 1: 'Yes'}

labels = [label_map.get(i, i) for i in t8.index]
sizes = t8.values

colors = ['#d40028', '#00a54c']

plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors)
plt.title('Heart Disease')
plt.show()

"""Pie Chart Stroke"""

t9 = cleaned_data['stroke'].value_counts()

print(t9)

label_map = {0: 'No', 1: 'Yes'}

labels = [label_map.get(i, i) for i in t9.index]
sizes = t9.values

colors = ['#d40028', '#00a54c']

plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors)
plt.title('Stroke')
plt.show()

"""Konklusi, dalam data kategorik hanya ada satu data yang memiliki error yaitu Gender dengan "Other". Data tersebut dihapuskan untuk menjaga kejernihan.

### Variabel Numerik (Box Plot)
"""

from google.colab import drive
drive.mount('/content/drive')

"""Untuk data Numerik, Box Plot digunakan untuk mengecheck outlier.

Box Plot Age
"""

plt.boxplot(stroke_data['age'])
plt.title('Boxplot of Age')
plt.ylabel('Age')
plt.grid(False)
plt.show()

"""Box Plot Average Glucose Level"""

plt.boxplot(stroke_data['avg_glucose_level'])
plt.title('Boxplot of Average Glucose Level')
plt.ylabel('Glucose Level')
plt.show()

original_count = stroke_data.shape[0]

filtered_data = stroke_data[stroke_data['avg_glucose_level'] <=144]

filtered_count = filtered_data.shape[0]

dropped_count = original_count - filtered_count

print(f"Original data count: {original_count}")
print(f"Filtered data count: {filtered_count}")
print(f"Number of rows dropped: {dropped_count}")

plt.boxplot(filtered_data['avg_glucose_level'])
plt.title('Boxplot of Average Glucose Level (Filtered)')
plt.ylabel('Glucose Level')
plt.grid(False)
plt.show()

"""Box Plot BMI"""

plt.boxplot(stroke_data['bmi'])
plt.title('BMI')
plt.ylabel('BMI')
plt.show()

"""Terjadi error dikarenakan didalam data terdapat data kosong, maka dari itu kita akan menghilangkan data kosong (N/A)."""

plt.boxplot(stroke_data['bmi'].dropna())
plt.title('BMI')
plt.ylabel('BMI')
plt.grid(False)
plt.show()

original_count = stroke_data.shape[0]
non_nan_count = stroke_data['bmi'].dropna().shape[0]
dropped_nan_count = original_count - non_nan_count

print(f"Original row count: {original_count}")
print(f"Rows with non-missing BMI: {non_nan_count}")
print(f"Rows dropped due to NaN in BMI: {dropped_nan_count}")

original_count = stroke_data.shape[0]

glucose_filtered = stroke_data[stroke_data['avg_glucose_level'] <= 144]

final_filtered = glucose_filtered[glucose_filtered['bmi'] <= 46]
final_count = final_filtered.shape[0]
dropped_count = original_count - final_count

print(f"Final row count after filtering avg_glucose_level and bmi: {final_count}")
print(f"Total rows dropped: {dropped_count}")

plt.boxplot(final_filtered['bmi'].dropna())
plt.title('Filtered BMI')
plt.ylabel('BMI')
plt.grid(False)
plt.show()

# cleaned_data = final_filtered

"""## Statistical Model

### 1. Scatter Plot with Linear Regression (Numeric Data)

Age vs Average Glucose Level
"""

sns.set_theme(style="whitegrid")

plt.figure(figsize=(8, 6))
sns.regplot(
    x='age',
    y='avg_glucose_level',
    data=cleaned_data,
    scatter_kws={'color': 'skyblue', 's': 60, 'edgecolor': 'black'},
    line_kws={'color': 'red'},
    ci=None  # equivalent to se = FALSE in R
)

plt.title("Age vs Average Glucose Level")
plt.xlabel("Age")
plt.ylabel("Average Glucose Level")
plt.tight_layout()
plt.show()

"""Dapat dilihat dari gambar diatas, korelasi antara umur dan rata-rata level glucose adalah semakin tua seseorang, semakin tinggi rata-rata kadar glucosa.

Age vs BMI
"""

sns.set_theme(style="whitegrid")

plt.figure(figsize=(8, 6))
sns.regplot(
    x='age',
    y='bmi',
    data=cleaned_data,
    scatter_kws={'color': 'skyblue', 's': 60, 'edgecolor': 'black'},
    line_kws={'color': 'red'},
    ci=None  # equivalent to se = FALSE in R
)

plt.title("Age vs BMI")
plt.xlabel("Age")
plt.ylabel("BMI")
plt.tight_layout()
plt.show()

"""Dapat dilihat dari gambar diatas, terdapat kemiringan keatas terhadap BMI dan umur yang dapat diartikan bahwa semakin tua seseorang semakin besar BMI mereka. Data terlihat melengkung yang berarti kebanyakan orang yang memiliki BMI tinggi adalah pada kalangan umur 40-an.

Average Glucose Level vs BMI
"""

sns.set_theme(style="whitegrid")

plt.figure(figsize=(8, 6))
sns.regplot(
    x='avg_glucose_level',
    y='bmi',
    data=cleaned_data,
    scatter_kws={'color': 'skyblue', 's': 60, 'edgecolor': 'black'},
    line_kws={'color': 'red'},
    ci=None  # equivalent to se = FALSE in R
)

plt.title("Average Glucose Level vs BMI")
plt.xlabel("Average Glucose Level")
plt.ylabel("BMI")
plt.tight_layout()
plt.show()

"""Dapat dilihat dari gambar diatas, korelasi antara rata rata kadar glucosa dan BMI bahwa adanya trend menaik, semakin besar kadar glucosa semakin tinggi BMI.

### 2. Chi Square Test (Categorical Data)
"""

categorical_columns = [
    'gender', 'ever_married', 'work_type', 'Residence_type',
    'smoking_status', 'hypertension', 'heart_disease'
]

for col in categorical_columns:

    contingency = pd.crosstab(cleaned_data[col], cleaned_data['stroke'])

    chi2, p, dof, expected = chi2_contingency(contingency)

    print(f"Chi-Square Test between '{col}' and 'stroke':")
    print(f"Chi2 = {chi2:.2f}, p-value = {p:.4f}, DoF = {dof}")
    if p < 0.05:
        print("→ Statistically significant (p < 0.05)")
    else:
        print("→ Not statistically significant (p ≥ 0.05)")
    print("-" * 50)

"""Jadi kita dapat membagi attribut kategorik berdasarkan kepengaruhan kepada data stroke:

Statistically Significant:
- Martial Status
- Work Type
- Smoking Status
- Hypertension
- Heart Disease

Statistically Insignificant:
- Gender
- Residence Type

## Machine Learning Model
"""

X = cleaned_data.iloc[:, [2, 3, 4, 8, 9]]
X.head()

Y = cleaned_data.loc[:,['stroke']]
Y.head()

from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn.metrics import classification_report


x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=42, stratify=Y)

x_train = x_train.dropna()

x_train.head()

x_test = x_test.dropna()

x_test.head()

y_train = y_train.loc[x_train.index]

y_train.head()

y_test = y_test.loc[x_test.index]

y_test.head()

len(x_test)

len(y_train)

len(y_test)

"""### 1. Decision Tree"""

from sklearn import tree
dt = tree.DecisionTreeClassifier()

"""#### Test Data Check"""

dt.fit(x_train,y_train)

prediksi_dt = dt.predict(x_test)
print(prediksi_dt)

tree.plot_tree(dt)

from matplotlib import pyplot as plt
fig = plt.figure(figsize=(10,10))
_ = tree.plot_tree(dt,
                   filled=True)

print(classification_report(y_test, prediksi_dt))

"""Kita bisa lihat bahwa accuracy sudah cukup tinggi, 93%, akan kita coba tinggikan dengan mengecheck max_depth"""

dt5 = tree.DecisionTreeClassifier(max_depth=5)

dt5.fit(x_train,y_train)

prediksi_dt5 = dt5.predict(x_test)

print(classification_report(y_test, prediksi_dt5))

"""Saat max_depth=5, akurasi menaik dari 93% < 95%


"""

dt6 = tree.DecisionTreeClassifier(max_depth=6)

dt6.fit(x_train,y_train)

prediksi_dt6 = dt6.predict(x_test)

print(classification_report(y_test, prediksi_dt6))

"""Saat max_depth=6, akurasi tetap sama 96%


"""

dt7 = tree.DecisionTreeClassifier(max_depth=7)

dt7.fit(x_train,y_train)

prediksi_dt7 = dt7.predict(x_test)

print(classification_report(y_test, prediksi_dt7))

"""Saat max_depth=7, akurasi menggurang, kembali ke 95%. 96% > 95%."""

dt4 = tree.DecisionTreeClassifier(max_depth=4)

dt4.fit(x_train,y_train)

prediksi_dt4 = dt4.predict(x_test)

print(classification_report(y_test, prediksi_dt4))

"""Saat max_depth=4, akurasi tetap sama 95%. Sama seperti max_depth=5, namun lebih kecil dari max_depth=6."""

dt3 = tree.DecisionTreeClassifier(max_depth=3)

dt3.fit(x_train,y_train)

prediksi_dt3 = dt3.predict(x_test)

print(classification_report(y_test, prediksi_dt3))

"""Saat max_depth=3, akurasi tetap 95%.

Jadi dapat dikonklusikan bahwa max_depth=6 sudah optimal.

#### Train Data Check
"""

dt.fit(x_test,y_test)

prediksi_dt = dt.predict(x_train)
print(prediksi_dt)

tree.plot_tree(dt)

from matplotlib import pyplot as plt
fig = plt.figure(figsize=(10,10))
_ = tree.plot_tree(dt,
                   filled=True)

print(classification_report(y_train, prediksi_dt))

"""Kita bisa lihat bahwa accuracy sudah cukup tinggi, 92%, akan kita coba tinggikan dengan mengecheck max_depth"""

dt15 = tree.DecisionTreeClassifier(max_depth=5)

dt15.fit(x_train, y_train)

prediksi_dt15 = dt15.predict(x_train)

print(classification_report(y_train, prediksi_dt15))

"""Saat max_depth=5, akurasi naik menjadi 96%. 92% < 96%."""

dt16 = tree.DecisionTreeClassifier(max_depth=6)

dt16.fit(x_train, y_train)

prediksi_dt16 = dt16.predict(x_train)

print(classification_report(y_train, prediksi_dt16))

"""Saat max_depth=6, akurasi tetap sama 96%."""

dt17 = tree.DecisionTreeClassifier(max_depth=7)

dt17.fit(x_train, y_train)

prediksi_dt17 = dt17.predict(x_train)

print(classification_report(y_train, prediksi_dt17))

"""Saat max_depth=7, akurasi tetap sama 96%."""

dt18 = tree.DecisionTreeClassifier(max_depth=8)

dt18.fit(x_train, y_train)

prediksi_dt18 = dt18.predict(x_train)

print(classification_report(y_train, prediksi_dt18))

"""Saat max_depth=8, akurasi naik menjadi 97%. 96% < 97%."""

dt19 = tree.DecisionTreeClassifier(max_depth=9)

dt19.fit(x_train, y_train)

prediksi_dt19 = dt19.predict(x_train)

print(classification_report(y_train, prediksi_dt19))

"""Saat max_depth=9, akurasi tetap sama 97%."""

dt20 = tree.DecisionTreeClassifier(max_depth=10)

dt20.fit(x_train, y_train)

prediksi_dt20 = dt20.predict(x_train)

print(classification_report(y_train, prediksi_dt20))

"""Saat max_depth=10, akurasi naik menjadi 98%. 97% < 98%."""

dt14 = tree.DecisionTreeClassifier(max_depth=4)

dt14.fit(x_train, y_train)

prediksi_dt14 = dt14.predict(x_train)

print(classification_report(y_train, prediksi_dt14))

"""Saat max_depth=4, akurasi tetap sama 96%."""

dt13 = tree.DecisionTreeClassifier(max_depth=3)

dt13.fit(x_train, y_train)

prediksi_dt13 = dt13.predict(x_train)

print(classification_report(y_train, prediksi_dt13))

"""Saat max_depth=3, akurasi tetap sama 96%."""

dt12 = tree.DecisionTreeClassifier(max_depth=2)

dt12.fit(x_train, y_train)

prediksi_dt12 = dt12.predict(x_train)

print(classification_report(y_train, prediksi_dt12))

"""Saat max_depth=2, akurasi tetap sama 96%.

Maka dapat disimpulkan bahwa max_depth=10, sudah paling optimal.

### 2. Random Forest
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

rf = RandomForestClassifier(n_estimators = 100, max_depth=2, random_state=0)

rf.fit(x_train, y_train)

prediksi_rf = rf.predict(x_test)
print(prediksi_rf)

yp = (prediksi_rf >= 0.5).astype(int)
yp

print(classification_report(y_test, yp))

"""Kita bisa lihat bahwa:

*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   f1-score: 98%

Kita akan coba mengubah n_estimatornya berserta max_depth-nya untuk meningkatkan presentasi.
"""

rf1 = RandomForestClassifier(n_estimators =50, max_depth= 50)

rf1.fit(x_train, y_train)

yp1 = rf1.predict(x_test)

print(classification_report(y_test, yp1, digits=2))

"""Saat di ubah menjadi n_estimator=50 dan max_depth=50:


*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   f1-score: 97%

"""

rf2 = RandomForestClassifier(n_estimators =40, max_depth= 50)

rf2.fit(x_train, y_train)

yp2 = rf2.predict(x_test)

print(classification_report(y_test, yp2, digits=2))

"""Saat di ubah menjadi n_estimator=40 dan max_depth=50:


*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   f1-score: 98%

"""

rf3 = RandomForestClassifier(n_estimators =60, max_depth= 50)

rf3.fit(x_train, y_train)

yp3 = rf3.predict(x_test)

print(classification_report(y_test, yp3, digits=2))

"""Saat di ubah menjadi n_estimator=60 dan max_depth=50:


*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   f1-score: 98%

"""

rf4 = RandomForestClassifier(n_estimators =50, max_depth= 40)

rf4.fit(x_train, y_train)

yp4 = rf4.predict(x_test)

print(classification_report(y_test, yp4, digits=2))

"""Saat di ubah menjadi n_estimator=50 dan max_depth=40:


*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   f1-score: 97%

"""

rf5 = RandomForestClassifier(n_estimators =50, max_depth= 60)

rf5.fit(x_train, y_train)

yp5 = rf5.predict(x_test)

print(classification_report(y_test, yp5, digits=2))

"""Saat di ubah menjadi n_estimator=50 dan max_depth=60:


*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   f1-score: 97%

"""

rf6 = RandomForestClassifier(n_estimators =30, max_depth= 50)

rf6.fit(x_train, y_train)

yp6 = rf6.predict(x_test)

print(classification_report(y_test, yp6, digits=2))

"""Saat di ubah menjadi n_estimator=30 dan max_depth=50:


*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   f1-score: 97%

"""

rf7 = RandomForestClassifier(n_estimators =70, max_depth= 50)

rf7.fit(x_train, y_train)

yp7 = rf7.predict(x_test)

print(classification_report(y_test, yp7, digits=2))

"""Saat di ubah menjadi n_estimator=70 dan max_depth=50:


*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   f1-score: 98%

"""

rf8 = RandomForestClassifier(n_estimators =50, max_depth= 70)

rf8.fit(x_train, y_train)

yp8 = rf8.predict(x_test)

print(classification_report(y_test, yp8, digits=2))

"""Saat di ubah menjadi n_estimator=50 dan max_depth=70:


*   Accuracy: 95%
*   Precision: 98%
*   Recall: 100%
*   f1-score: 97%

"""

rf9 = RandomForestClassifier(n_estimators =50, max_depth= 30)

rf9.fit(x_train, y_train)

yp9 = rf9.predict(x_test)

print(classification_report(y_test, yp5, digits=2))

"""Saat di ubah menjadi n_estimator=50 dan max_depth=30:


*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   f1-score: 97%

Dapat di konklusikan bahwa n_estimator=4 dan max_depth=5 sudah paling optimal.

## Neural Network
"""

cleaned_data.head()

"""### Relu

#### hidden_layer_sizes = 3
"""

from sklearn.neural_network import MLPClassifier

nn_clf_relu = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=3, activation='relu')

nn_clf_relu.fit(x_train, y_train)

y_hat_relu_3 = nn_clf_relu.predict(x_test)

print(classification_report(y_test, y_hat_relu_3, digits=2))

"""*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   F1-Score: 98%

#### hidden_layer_sizes = 4
"""

nn_clf_relu = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=4, activation='relu')

nn_clf_relu.fit(x_train, y_train)

y_hat_relu_4 = nn_clf_relu.predict(x_test)

print(classification_report(y_test, y_hat_relu_4, digits=2))

"""*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   F1-Score: 98%

#### hidden_layer_sizes = 5
"""

nn_clf_relu = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=5, activation='relu')

nn_clf_relu.fit(x_train, y_train)

y_hat_relu_5 = nn_clf_relu.predict(x_test)

print(classification_report(y_test, y_hat_relu_5, digits=2))

"""*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   F1-Score: 98%

#### hidden_layer_sizes = 6
"""

nn_clf_relu = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=6, activation='relu')

nn_clf_relu.fit(x_train, y_train)

y_hat_relu_6 = nn_clf_relu.predict(x_test)

print(classification_report(y_test, y_hat_relu_6, digits=2))

"""*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   F1-Score: 98%

#### hidden_layer_sizes = 7
"""

nn_clf_relu = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=7, activation='relu')

nn_clf_relu.fit(x_train, y_train)

y_hat_relu_7 = nn_clf_relu.predict(x_test)

print(classification_report(y_test, y_hat_relu_7, digits=2))

"""*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   F1-Score: 97%

### Tanh

#### hidden_layer_sizes = 3
"""

nn_clf_tanh = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=3, activation='tanh')

nn_clf_tanh.fit(x_train, y_train)

y_hat_tanh_3 = nn_clf_tanh.predict(x_test)

print(classification_report(y_test, y_hat_tanh_3, digits=2))

"""*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   F1-Score: 98%

#### hidden_layer_sizes = 4
"""

nn_clf_tanh = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=4, activation='tanh')

nn_clf_tanh.fit(x_train, y_train)

y_hat_tanh_4 = nn_clf_tanh.predict(x_test)

print(classification_report(y_test, y_hat_tanh_4, digits=2))

"""*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   F1-Score: 98%

#### hidden_layer_sizes = 5
"""

nn_clf_tanh = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=5, activation='tanh')

nn_clf_tanh.fit(x_train, y_train)

y_hat_tanh_5 = nn_clf_tanh.predict(x_test)

print(classification_report(y_test, y_hat_tanh_5, digits=2))

"""*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   F1-Score: 98%

#### hidden_layer_sizes = 6
"""

nn_clf_tanh = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=6, activation='tanh')

nn_clf_tanh.fit(x_train, y_train)

y_hat_tanh_6 = nn_clf_tanh.predict(x_test)

print(classification_report(y_test, y_hat_tanh_6, digits=2))

"""*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   F1-Score: 98%

#### hidden_layer_sizes = 7
"""

nn_clf_tanh = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=7, activation='tanh')

nn_clf_tanh.fit(x_train, y_train)

y_hat_tanh_7 = nn_clf_tanh.predict(x_test)

print(classification_report(y_test, y_hat_tanh_7, digits=2))

"""*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   F1-Score: 98%

### Logistic

#### hidden_layer_sizes = 4
"""

nn_clf_log = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=4, activation='logistic')

nn_clf_log.fit(x_train, y_train)

y_hat_log_4 = nn_clf_log.predict(x_test)

print(classification_report(y_test, y_hat_log_4, digits=2))

"""*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   F1-Score: 98%

#### hidden_layer_sizes = 5
"""

nn_clf_log = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=5, activation='logistic')

nn_clf_log.fit(x_train, y_train)

y_hat_log_5 = nn_clf_log.predict(x_test)

print(classification_report(y_test, y_hat_log_5, digits=2))

"""*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   F1-Score: 98%

#### hidden_layer_sizes = 6
"""

nn_clf_log = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=6, activation='logistic')

nn_clf_log.fit(x_train, y_train)

y_hat_log_6 = nn_clf_log.predict(x_test)

print(classification_report(y_test, y_hat_log_6, digits=2))

"""*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   F1-Score: 98%

### Indentity

#### hidden_layer_sizes = 4
"""

nn_clf_id = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=4, activation='identity')

nn_clf_id.fit(x_train, y_train)

y_hat_id_4 = nn_clf_id.predict(x_test)

print(classification_report(y_test, y_hat_id_4, digits=2))

"""*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   F1-Score: 97%

#### hidden_layer_sizes = 5
"""

nn_clf_id = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=5, activation='identity')

nn_clf_id.fit(x_train, y_train)

y_hat_id_5 = nn_clf_id.predict(x_test)

print(classification_report(y_test, y_hat_id_5, digits=2))

"""*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   F1-Score: 98%

#### hidden_layer_sizes = 6
"""

nn_clf_id = MLPClassifier(random_state=1, max_iter=5000, hidden_layer_sizes=6, activation='identity')

nn_clf_id.fit(x_train, y_train)

y_hat_id_6 = nn_clf_id.predict(x_test)

print(classification_report(y_test, y_hat_id_6, digits=2))

"""*   Accuracy: 95%
*   Precision: 95%
*   Recall: 100%
*   F1-Score: 98%

## Neural Network With Multiple Hidden Layer

### Relu

#### 5,6
"""

nn2_clf_relu = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(5,6), activation='relu')

nn2_clf_relu.fit(x_train, y_train)

y_hat2_relu = nn2_clf_relu.predict(x_test)

print(classification_report(y_test, y_hat2_relu, digits=2))

"""#### 6,5"""

nn2_clf_relu = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(6,5), activation='relu')

nn2_clf_relu.fit(x_train, y_train)

y_hat2_relu = nn2_clf_relu.predict(x_test)

print(classification_report(y_test, y_hat2_relu, digits=2))

"""#### 5,5"""

nn2_clf_relu = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(5,5), activation='relu')

nn2_clf_relu.fit(x_train, y_train)

y_hat2_relu = nn2_clf_relu.predict(x_test)

print(classification_report(y_test, y_hat2_relu, digits=2))

"""#### 4,5"""

nn2_clf_relu = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(4,5), activation='relu')

nn2_clf_relu.fit(x_train, y_train)

y_hat2_relu = nn2_clf_relu.predict(x_test)

print(classification_report(y_test, y_hat2_relu, digits=2))

"""#### 5,4"""

nn2_clf_relu = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(5,4), activation='relu')

nn2_clf_relu.fit(x_train, y_train)

y_hat2_relu = nn2_clf_relu.predict(x_test)

print(classification_report(y_test, y_hat2_relu, digits=2))

"""### Tanh

#### 5,6
"""

nn2_clf_tanh = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(5,6), activation='tanh')

nn2_clf_tanh.fit(x_train, y_train)

y_hat2_tanh = nn2_clf_tanh.predict(x_test)

print(classification_report(y_test, y_hat2_tanh, digits=2))

"""#### 6,5"""

nn2_clf_tanh = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(6,5), activation='tanh')

nn2_clf_tanh.fit(x_train, y_train)

y_hat2_tanh = nn2_clf_tanh.predict(x_test)

print(classification_report(y_test, y_hat2_tanh, digits=2))

"""#### 5,5"""

nn2_clf_tanh = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(5,5), activation='tanh')

nn2_clf_tanh.fit(x_train, y_train)

y_hat2_tanh = nn2_clf_tanh.predict(x_test)

print(classification_report(y_test, y_hat2_tanh, digits=2))

"""#### 4,5"""

nn2_clf_tanh = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(4,5), activation='tanh')

nn2_clf_tanh.fit(x_train, y_train)

y_hat2_tanh = nn2_clf_tanh.predict(x_test)

print(classification_report(y_test, y_hat2_tanh, digits=2))

"""#### 5,4"""

nn2_clf_tanh = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(5,4), activation='tanh')

nn2_clf_tanh.fit(x_train, y_train)

y_hat2_tanh = nn2_clf_tanh.predict(x_test)

print(classification_report(y_test, y_hat2_tanh, digits=2))

"""### Logistic

#### 5,6
"""

nn2_clf_log = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(5,6), activation='logistic')

nn2_clf_log.fit(x_train, y_train)

y_hat2_log = nn2_clf_log.predict(x_test)

print(classification_report(y_test, y_hat2_log, digits=2))

"""#### 6,5"""

nn2_clf_log = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(6,5), activation='logistic')

nn2_clf_log.fit(x_train, y_train)

y_hat2_log = nn2_clf_log.predict(x_test)

print(classification_report(y_test, y_hat2_log, digits=2))

"""#### 5,5"""

nn2_clf_log = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(5,5), activation='logistic')

nn2_clf_log.fit(x_train, y_train)

y_hat2_log = nn2_clf_log.predict(x_test)

print(classification_report(y_test, y_hat2_log, digits=2))

"""#### 4,5"""

nn2_clf_log = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(4,5), activation='logistic')

nn2_clf_log.fit(x_train, y_train)

y_hat2_log = nn2_clf_log.predict(x_test)

print(classification_report(y_test, y_hat2_log, digits=2))

"""#### 5,4"""

nn2_clf_log = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(5,4), activation='logistic')

nn2_clf_log.fit(x_train, y_train)

y_hat2_log = nn2_clf_log.predict(x_test)

print(classification_report(y_test, y_hat2_log, digits=2))

"""### Identity

#### 5,6
"""

nn2_clf_id = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(5,6), activation='identity')

nn2_clf_id.fit(x_train, y_train)

y_hat2_id = nn2_clf_id.predict(x_test)

print(classification_report(y_test, y_hat2_id, digits=2))

"""#### 6,5"""

nn2_clf_id = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(6,5), activation='identity')

nn2_clf_id.fit(x_train, y_train)

y_hat2_id = nn2_clf_id.predict(x_test)

print(classification_report(y_test, y_hat2_id, digits=2))

"""#### 5,5"""

nn2_clf_id = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(5,5), activation='identity')

nn2_clf_id.fit(x_train, y_train)

y_hat2_id = nn2_clf_id.predict(x_test)

print(classification_report(y_test, y_hat2_id, digits=2))

"""#### 4,5"""

nn2_clf_id = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(4,5), activation='identity')

nn2_clf_id.fit(x_train, y_train)

y_hat2_id = nn2_clf_id.predict(x_test)

print(classification_report(y_test, y_hat2_id, digits=2))

"""#### 5,4"""

nn2_clf_id = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(5,4), activation='identity')

nn2_clf_id.fit(x_train, y_train)

y_hat2_id = nn2_clf_id.predict(x_test)

print(classification_report(y_test, y_hat2_id, digits=2))

"""#### 4,4"""

nn2_clf_id = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(4,4), activation='identity')

nn2_clf_id.fit(x_train, y_train)

y_hat2_id = nn2_clf_id.predict(x_test)

print(classification_report(y_test, y_hat2_id, digits=2))

"""#### 4,3"""

nn2_clf_id = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(4,3), activation='identity')

nn2_clf_id.fit(x_train, y_train)

y_hat2_id = nn2_clf_id.predict(x_test)

print(classification_report(y_test, y_hat2_id, digits=2))

"""(4,4) sudah paling optimal



"""